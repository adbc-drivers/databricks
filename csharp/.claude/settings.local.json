{
  "permissions": {
    "allow": [
      "Bash(git stack:*)",
      "WebSearch",
      "Bash(dotnet build:*)",
      "Bash(dotnet test:*)",
      "Bash(git add:*)",
      "Bash(git commit:*)",
      "Bash(gh pr edit 126 --repo adbc-drivers/databricks --body \"$\\(cat <<''EOF''\n## ðŸ¥ž Stacked PR\nUse this [link]\\(https://github.com/adbc-drivers/databricks/pull/126/files\\) to review incremental changes.\n- [**stack/jade.wang/PECO-2524-row-count-limiting**]\\(https://github.com/adbc-drivers/databricks/pull/126\\) [[Files changed]\\(https://github.com/adbc-drivers/databricks/pull/126/files\\)]\n\n---------\n\n## Summary\n\nWhen the Databricks server uses `trimArrowBatchesToLimit=false` \\(the default\\), it may return more data than the LIMIT in the last batch but reports adjusted `rowCount` values in the batch/chunk metadata so that `sum\\(rowCount\\)` equals the actual limit.\n\nThis fix updates both `DatabricksReader` \\(inline results\\) and `CloudFetchReader` \\(CloudFetch results\\) to track the reported `rowCount` from metadata and trim excess rows from the actual Arrow data when needed.\n\n## Changes\n\n- **DatabricksReader**: Track `_currentBatchExpectedRows` from `TSparkArrowBatch.RowCount` and slice the `RecordBatch` if it exceeds the expected count\n- **CloudFetchReader**: Track `_currentChunkExpectedRows` and `_currentChunkRowsRead` from `IDownloadResult.RowCount` and trim when cumulative rows exceed the chunk''s limit\n- **CloudFetchE2ETest**: Update assertion to validate exact row count for Thrift protocol\n\n## Test plan\n\n- [x] Build passes: `dotnet build AdbcDrivers.Databricks.csproj`\n- [x] All CloudFetch E2E tests pass \\(16/16\\): `dotnet test --filter \"CloudFetchE2ETest.TestCloudFetch\"`\n- [x] Tests validate exact row count for Thrift protocol LIMIT queries\n\nðŸ¤– Generated with [Claude Code]\\(https://claude.ai/code\\)\n\nCloses PECO-2524\nEOF\n\\)\")",
      "Bash(gh auth switch:*)",
      "Bash(gh auth:*)",
      "mcp__jira__jira_write_api_call"
    ]
  }
}
